{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddlePaddle BYOS\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local` (or `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance).\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "This notebook was created and tested on a single ml.p2.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-PaddleNLP-DuUIE'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# instance_type = 'local'\n",
    "\n",
    "# if subprocess.call('nvidia-smi') == 0:\n",
    "#     ## Set type to GPU if one is present\n",
    "#     instance_type = 'local_gpu'\n",
    "    \n",
    "# print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = 'file:///home/ec2-user/SageMaker/paddlenlp_sagemaker/data/'\n",
    "# inputs = {'training': base_dir}\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# # git_config = {'repo': 'https://github.com/PaddlePaddle/PaddleNLP.git', 'branch': 'develop'}\n",
    "\n",
    "# hyperparameters = {'train_path': '/opt/ml/input/data/training/train.txt', \n",
    "#                    'dev_path': '/opt/ml/input/data/training/dev.txt', \n",
    "#                    'save_dir': '/opt/ml/model', \n",
    "#                    'learning_rate': 1e-5, \n",
    "#                    'batch_size': 16, \n",
    "#                    'max_seq_len':512, \n",
    "#                    'num_epochs': 100, \n",
    "#                    'model': 'uie-base',\n",
    "#                    'seed': 1000,\n",
    "#                    'logging_steps': 10,\n",
    "#                    'valid_steps': 100,\n",
    "#                    'device': 'gpu'}\n",
    "\n",
    "# estimator = PyTorch(entry_point='finetune.py',\n",
    "#                             source_dir='./',\n",
    "# #                             source_dir='model_zoo/uie/',\n",
    "# #                             git_config=git_config,\n",
    "#                             role=role,\n",
    "#                             hyperparameters=hyperparameters,\n",
    "#                             framework_version='1.9.1',\n",
    "#                             py_version='py38',\n",
    "#                             script_mode=True,\n",
    "#                             instance_count=1,  # 1 or 2 or ...\n",
    "#                             instance_type=instance_type)\n",
    "\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training using GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 's3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE'}\n"
     ]
    }
   ],
   "source": [
    "WORK_DIRECTORY = '/home/ec2-user/SageMaker/DuUIE/data/'\n",
    "\n",
    "# data_location = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "data_location = 's3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE'\n",
    "\n",
    "inputs = {'training': data_location}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: config/.ipynb_checkpoints/multi-task-duuie-checkpoint.yaml to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/config/.ipynb_checkpoints/multi-task-duuie-checkpoint.yaml\n",
      "upload: config/multi-task-duuie.yaml to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/config/multi-task-duuie.yaml\n",
      "upload: uie-char-small/vocab.txt to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/uie-char-small/vocab.txt\n",
      "upload: uie-char-small/model_config.json to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/uie-char-small/model_config.json\n",
      "upload: uie-char-small/tokenizer_config.json to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/uie-char-small/tokenizer_config.json\n",
      "upload: uie-char-small/model_state.pdparams to s3://sagemaker-us-east-1-579019700964/sagemaker/DEMO-PaddleNLP-DuUIE/uie-char-small/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp --recursive config $data_location/config\n",
    "# !aws s3 cp --recursive uie-char-small $data_location/uie-char-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE .ipynb_checkpoints/\n",
      "                           PRE config/\n",
      "                           PRE duuie/\n",
      "                           PRE duuie_pre/\n",
      "                           PRE seen_schema/\n",
      "                           PRE uie-char-small/\n",
      "2022-06-09 07:53:20  102605243 duuie.zip\n",
      "2022-06-09 07:53:20    1979357 duuie_test_a.json\n",
      "2022-06-09 07:53:20     798571 duuie_test_a.zip\n",
      "2022-06-09 07:53:20       6846 seen_schema.zip\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 ls $data_location/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 10:16:25 Starting - Starting the training job...\n",
      "2022-06-09 10:16:51 Starting - Preparing the instances for trainingProfilerReport-1654769785: InProgress\n",
      ".........\n",
      "2022-06-09 10:18:15 Downloading - Downloading input data...\n",
      "2022-06-09 10:18:50 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-09 10:23:05,414 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-09 10:23:05,440 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-09 10:23:05,448 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-09 10:23:06,074 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting paddlepaddle-gpu\u001b[0m\n",
      "\u001b[34mDownloading paddlepaddle_gpu-2.3.0-cp38-cp38-manylinux1_x86_64.whl (576.1 MB)\u001b[0m\n",
      "\n",
      "2022-06-09 10:23:10 Training - Training image download completed. Training in progress.\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 576.1/576.1 MB 2.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib_metadata in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.11.4)\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 65.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting visualdl\u001b[0m\n",
      "\u001b[34mDownloading visualdl-2.2.3-py3-none-any.whl (2.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 52.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddlenlp\u001b[0m\n",
      "\u001b[34mDownloading paddlenlp-2.3.3-py3-none-any.whl (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 58.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (5.1.1)\u001b[0m\n",
      "\u001b[34mCollecting astor\u001b[0m\n",
      "\u001b[34mDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting paddle-bfloat==0.1.2\u001b[0m\n",
      "\u001b[34mDownloading paddle_bfloat-0.1.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (378 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 378.1/378.1 kB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum==3.3.0\u001b[0m\n",
      "\u001b[34mDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 13.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (1.22.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (9.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (3.20.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib_metadata->-r requirements.txt (line 3)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 4)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.9/764.9 kB 50.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 4)) (4.61.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 4)) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting shellcheck-py\u001b[0m\n",
      "\u001b[34mDownloading shellcheck_py-0.8.0.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 72.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8>=3.7.9\u001b[0m\n",
      "\u001b[34mDownloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.1/64.1 kB 4.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bce-python-sdk\u001b[0m\n",
      "\u001b[34mDownloading bce_python_sdk-0.8.73-py3-none-any.whl (204 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 204.5/204.5 kB 34.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flask>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from visualdl->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting pre-commit\u001b[0m\n",
      "\u001b[34mDownloading pre_commit-2.19.0-py2.py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.3/199.3 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from visualdl->-r requirements.txt (line 5)) (3.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from visualdl->-r requirements.txt (line 5)) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting Flask-Babel>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess<=0.70.12.2\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddlefsl\u001b[0m\n",
      "\u001b[34mDownloading paddlefsl-1.1.0-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.0/101.0 kB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddle2onnx\u001b[0m\n",
      "\u001b[34mDownloading paddle2onnx-0.9.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 63.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 49.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 8.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.2.2-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.8/346.8 kB 34.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 6)) (0.4.4)\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.5\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting protobuf>=3.1.0\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 51.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 6)) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 14.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 6)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 6)) (21.2)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 14.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes<2.5.0,>=2.4.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.7/69.7 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle<2.9.0,>=2.8.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 kB 8.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 5)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 5)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from Flask-Babel>=1.0.0->visualdl->-r requirements.txt (line 5)) (2021.3)\u001b[0m\n",
      "\u001b[34mCollecting Babel>=2.3\u001b[0m\n",
      "\u001b[34mDownloading Babel-2.10.1-py3-none-any.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/9.5 MB 75.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 1)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from bce-python-sdk->visualdl->-r requirements.txt (line 5)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodome>=3.8.0\u001b[0m\n",
      "\u001b[34mDownloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 69.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->visualdl->-r requirements.txt (line 5)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->visualdl->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->visualdl->-r requirements.txt (line 5)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->visualdl->-r requirements.txt (line 5)) (1.4.2)\u001b[0m\n",
      "\u001b[34mCollecting cfgv>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting identify>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading identify-2.5.1-py2.py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 15.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting toml\u001b[0m\n",
      "\u001b[34mDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from pre-commit->visualdl->-r requirements.txt (line 5)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting nodeenv>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting virtualenv>=20.0.8\u001b[0m\n",
      "\u001b[34mDownloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 68.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval->paddlenlp->-r requirements.txt (line 6)) (1.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 6)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 6)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 6)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting distlib<1,>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 461.2/461.2 kB 41.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting platformdirs<3,>=2\u001b[0m\n",
      "\u001b[34mDownloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 32.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 23.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 6)) (21.4.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: jieba, seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=becd8aacec625e52cdca8f5d1274b82900b7a30c2c8a8d864a28f31b47984026\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=2b2becc9cb7ef4dfb17413af036d713f614e74733989ac398b68f6965794cba4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\u001b[0m\n",
      "\u001b[34mSuccessfully built jieba seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, paddle2onnx, paddle-bfloat, nodeenv, mccabe, jieba, distlib, xxhash, tqdm, toml, shellcheck-py, regex, pyflakes, pycryptodome, pycodestyle, protobuf, platformdirs, opt-einsum, multidict, identify, frozenlist, filelock, dill, colorlog, cfgv, Babel, async-timeout, astor, yarl, virtualenv, responses, paddlepaddle-gpu, paddlefsl, nltk, multiprocess, huggingface-hub, flake8, bce-python-sdk, aiosignal, seqeval, pre-commit, Flask-Babel, aiohttp, visualdl, datasets, paddlenlp\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.61.2\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.61.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.20.1\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.20.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.20.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.5.1\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.5.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.5.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.13\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.13:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.13\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Babel-2.10.1 Flask-Babel-2.0.0 aiohttp-3.8.1 aiosignal-1.2.0 astor-0.8.1 async-timeout-4.0.2 bce-python-sdk-0.8.73 cfgv-3.3.1 colorlog-6.6.0 datasets-2.2.2 dill-0.3.4 distlib-0.3.4 filelock-3.7.1 flake8-4.0.1 frozenlist-1.3.0 huggingface-hub-0.7.0 identify-2.5.1 jieba-0.42.1 mccabe-0.6.1 multidict-6.0.2 multiprocess-0.70.12.2 nltk-3.7 nodeenv-1.6.0 opt-einsum-3.3.0 paddle-bfloat-0.1.2 paddle2onnx-0.9.7 paddlefsl-1.1.0 paddlenlp-2.3.3 paddlepaddle-gpu-2.3.0 platformdirs-2.5.2 pre-commit-2.19.0 protobuf-3.20.0 pycodestyle-2.8.0 pycryptodome-3.14.1 pyflakes-2.4.0 regex-2022.6.2 responses-0.18.0 sentencepiece-0.1.96 seqeval-1.2.2 shellcheck-py-0.8.0.4 toml-0.10.2 tqdm-4.64.0 virtualenv-20.14.1 visualdl-2.2.3 xxhash-3.0.0 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:12,648 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:12,648 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:12,714 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"device\": \"gpu\",\n",
      "        \"do_train\": \"\",\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"learning_rate\": 0.0005,\n",
      "        \"logging_dir\": \"/opt/ml/output/duuie_multi_task_b32_lr5e-4_log\",\n",
      "        \"metric_for_best_model\": \"all-task-ave\",\n",
      "        \"model_name_or_path\": \"/opt/ml/input/data/training/uie-char-small\",\n",
      "        \"multi_task_config\": \"/opt/ml/input/data/training/config/multi-task-duuie.yaml\",\n",
      "        \"negative_keep\": 1.0,\n",
      "        \"num_train_epochs\": 10,\n",
      "        \"output_dir\": \"/opt/ml/model/duuie_multi_task_b32_lr5e-4\",\n",
      "        \"overwrite_output_dir\": \"\",\n",
      "        \"per_device_eval_batch_size\": 128,\n",
      "        \"per_device_train_batch_size\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-06-09-10-16-24-907\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-579019700964/pytorch-training-2022-06-09-10-16-24-907/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_seq2struct\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_seq2struct.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"device\":\"gpu\",\"do_train\":\"\",\"gradient_accumulation_steps\":1,\"learning_rate\":0.0005,\"logging_dir\":\"/opt/ml/output/duuie_multi_task_b32_lr5e-4_log\",\"metric_for_best_model\":\"all-task-ave\",\"model_name_or_path\":\"/opt/ml/input/data/training/uie-char-small\",\"multi_task_config\":\"/opt/ml/input/data/training/config/multi-task-duuie.yaml\",\"negative_keep\":1.0,\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model/duuie_multi_task_b32_lr5e-4\",\"overwrite_output_dir\":\"\",\"per_device_eval_batch_size\":128,\"per_device_train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_seq2struct.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_seq2struct\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-579019700964/pytorch-training-2022-06-09-10-16-24-907/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"device\":\"gpu\",\"do_train\":\"\",\"gradient_accumulation_steps\":1,\"learning_rate\":0.0005,\"logging_dir\":\"/opt/ml/output/duuie_multi_task_b32_lr5e-4_log\",\"metric_for_best_model\":\"all-task-ave\",\"model_name_or_path\":\"/opt/ml/input/data/training/uie-char-small\",\"multi_task_config\":\"/opt/ml/input/data/training/config/multi-task-duuie.yaml\",\"negative_keep\":1.0,\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model/duuie_multi_task_b32_lr5e-4\",\"overwrite_output_dir\":\"\",\"per_device_eval_batch_size\":128,\"per_device_train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-06-09-10-16-24-907\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-579019700964/pytorch-training-2022-06-09-10-16-24-907/source/sourcedir.tar.gz\",\"module_name\":\"run_seq2struct\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_seq2struct.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--device\",\"gpu\",\"--do_train\",\"\",\"--gradient_accumulation_steps\",\"1\",\"--learning_rate\",\"0.0005\",\"--logging_dir\",\"/opt/ml/output/duuie_multi_task_b32_lr5e-4_log\",\"--metric_for_best_model\",\"all-task-ave\",\"--model_name_or_path\",\"/opt/ml/input/data/training/uie-char-small\",\"--multi_task_config\",\"/opt/ml/input/data/training/config/multi-task-duuie.yaml\",\"--negative_keep\",\"1.0\",\"--num_train_epochs\",\"10\",\"--output_dir\",\"/opt/ml/model/duuie_multi_task_b32_lr5e-4\",\"--overwrite_output_dir\",\"\",\"--per_device_eval_batch_size\",\"128\",\"--per_device_train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DEVICE=gpu\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0005\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_DIR=/opt/ml/output/duuie_multi_task_b32_lr5e-4_log\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=all-task-ave\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=/opt/ml/input/data/training/uie-char-small\u001b[0m\n",
      "\u001b[34mSM_HP_MULTI_TASK_CONFIG=/opt/ml/input/data/training/config/multi-task-duuie.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_NEGATIVE_KEEP=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model/duuie_multi_task_b32_lr5e-4\u001b[0m\n",
      "\u001b[34mSM_HP_OVERWRITE_OUTPUT_DIR=\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python run_seq2struct.py --device gpu --do_train  --gradient_accumulation_steps 1 --learning_rate 0.0005 --logging_dir /opt/ml/output/duuie_multi_task_b32_lr5e-4_log --metric_for_best_model all-task-ave --model_name_or_path /opt/ml/input/data/training/uie-char-small --multi_task_config /opt/ml/input/data/training/config/multi-task-duuie.yaml --negative_keep 1.0 --num_train_epochs 10 --output_dir /opt/ml/model/duuie_multi_task_b32_lr5e-4 --overwrite_output_dir  --per_device_eval_batch_size 128 --per_device_train_batch_size 16\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,042 - __main__ - INFO - Namespace(adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, asoc_noise=0, dataloader_num_workers=0, device='gpu', do_eval=False, do_train=True, gradient_accumulation_steps=1, ignore_pad_token_for_loss=True, learning_rate=0.0005, logging_dir='/opt/ml/output/duuie_multi_task_b32_lr5e-4_log', logging_steps=500, lr_scheduler_type='linear', max_grad_norm=1.0, max_prefix_length=None, max_source_length=384, max_steps=-1, max_target_length=192, meta_negative=-1, meta_positive_rate=1, metric_for_best_model='all-task-ave', model_name_or_path='/opt/ml/input/data/training/uie-char-small', multi_task_config='/opt/ml/input/data/training/config/multi-task-duuie.yaml', negative_keep=1.0, num_train_epochs=10, ordered_prompt=False, output_dir='/opt/ml/model/duuie_multi_task_b32_lr5e-4', overwrite_output_dir=True, per_device_eval_batch_size=128, per_device_train_batch_size=16, scale_loss=32768, seed=42, spot_noise=0, use_amp=False, warmup_ratio=0.06, warmup_steps=-1, weight_decay=0.0, writer_type='visualdl')\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,042 - __main__ - INFO - **********  Configuration Arguments **********\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,042 - __main__ - INFO - adam_beta1: 0.9\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,042 - __main__ - INFO - adam_beta2: 0.999\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,042 - __main__ - INFO - adam_epsilon: 1e-06\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - asoc_noise: 0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - dataloader_num_workers: 0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - device: gpu\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - do_eval: False\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - do_train: True\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - gradient_accumulation_steps: 1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - ignore_pad_token_for_loss: True\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - learning_rate: 0.0005\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - logging_dir: /opt/ml/output/duuie_multi_task_b32_lr5e-4_log\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - logging_steps: 500\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - lr_scheduler_type: linear\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,043 - __main__ - INFO - max_grad_norm: 1.0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - max_prefix_length: None\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - max_source_length: 384\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - max_steps: -1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - max_target_length: 192\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - meta_negative: -1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - meta_positive_rate: 1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - metric_for_best_model: all-task-ave\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - model_name_or_path: /opt/ml/input/data/training/uie-char-small\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - multi_task_config: /opt/ml/input/data/training/config/multi-task-duuie.yaml\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - negative_keep: 1.0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,044 - __main__ - INFO - num_train_epochs: 10\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - ordered_prompt: False\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - output_dir: /opt/ml/model/duuie_multi_task_b32_lr5e-4\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - overwrite_output_dir: True\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - per_device_eval_batch_size: 128\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - per_device_train_batch_size: 16\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - scale_loss: 32768\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - seed: 42\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - spot_noise: 0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - use_amp: False\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - warmup_ratio: 0.06\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - warmup_steps: -1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,045 - __main__ - INFO - weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,046 - __main__ - INFO - writer_type: visualdl\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:20,046 - __main__ - INFO - **********************************************\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,065] [    INFO]#033[0m - Adding <extra_id_0> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,065] [    INFO]#033[0m - Adding <extra_id_1> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,065] [    INFO]#033[0m - Adding <extra_id_2> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,066] [    INFO]#033[0m - Adding <extra_id_3> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,066] [    INFO]#033[0m - Adding <extra_id_4> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,066] [    INFO]#033[0m - Adding <extra_id_5> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,066] [    INFO]#033[0m - Adding <extra_id_6> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,066] [    INFO]#033[0m - Adding <extra_id_7> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_8> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_9> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_10> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_11> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_12> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,067] [    INFO]#033[0m - Adding <extra_id_13> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_14> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_15> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_16> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_17> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_18> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,068] [    INFO]#033[0m - Adding <extra_id_19> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_20> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_21> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_22> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_23> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_24> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_25> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,069] [    INFO]#033[0m - Adding <extra_id_26> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_27> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_28> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_29> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_30> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_31> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,070] [    INFO]#033[0m - Adding <extra_id_32> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_33> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_34> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_35> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_36> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_37> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,071] [    INFO]#033[0m - Adding <extra_id_38> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_39> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_40> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_41> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_42> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_43> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_44> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,072] [    INFO]#033[0m - Adding <extra_id_45> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_46> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_47> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_48> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_49> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_50> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,073] [    INFO]#033[0m - Adding <extra_id_51> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,074] [    INFO]#033[0m - Adding <extra_id_52> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,074] [    INFO]#033[0m - Adding <extra_id_53> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,074] [    INFO]#033[0m - Adding <extra_id_54> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,074] [    INFO]#033[0m - Adding <extra_id_55> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_56> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_57> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_58> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_59> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_60> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,075] [    INFO]#033[0m - Adding <extra_id_61> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_62> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_63> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_64> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_65> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_66> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,076] [    INFO]#033[0m - Adding <extra_id_67> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_68> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_69> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_70> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_71> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_72> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,077] [    INFO]#033[0m - Adding <extra_id_73> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_74> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_75> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_76> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_77> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_78> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,078] [    INFO]#033[0m - Adding <extra_id_79> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_80> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_81> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_82> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_83> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_84> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,079] [    INFO]#033[0m - Adding <extra_id_85> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_86> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_87> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_88> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_89> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_90> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,080] [    INFO]#033[0m - Adding <extra_id_91> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_92> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_93> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_94> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_95> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_96> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_97> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,081] [    INFO]#033[0m - Adding <extra_id_98> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,082] [    INFO]#033[0m - Adding <extra_id_99> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,082] [    INFO]#033[0m - Adding <spot> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,082] [    INFO]#033[0m - Adding <asoc> to the vocabulary#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 10:24:20,083] [    INFO]#033[0m - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.#033[0m\u001b[0m\n",
      "\u001b[34mW0609 10:24:20.085703    86 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 10.2\u001b[0m\n",
      "\u001b[34mW0609 10:24:20.117028    86 gpu_context.cc:306] device: 0, cuDNN Version: 8.0.\u001b[0m\n",
      "\u001b[34m2022-06-09 10:24:32,446 - __main__ - INFO - Load data according to /opt/ml/input/data/training/config/multi-task-duuie.yaml ...\u001b[0m\n",
      "\u001b[34m2022-06-09 10:25:25,736 - __main__ - INFO - Load 69490 instances from /opt/ml/input/data/training/duuie_pre/DUIE_LIFE_SPO/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:26:18,682 - __main__ - INFO - Load 69490 instances from /opt/ml/input/data/training/duuie_pre/DUIE_ORG_SPO/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:26:26,139 - __main__ - INFO - Load 11908 instances from /opt/ml/input/data/training/duuie_pre/体育竞赛/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:26:33,462 - __main__ - INFO - Load 11908 instances from /opt/ml/input/data/training/duuie_pre/灾害意外/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:26:37,747 - __main__ - INFO - Load 2400 instances from /opt/ml/input/data/training/duuie_pre/CONV-ASA/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:02,728 - __main__ - INFO - Load 45000 instances from /opt/ml/input/data/training/duuie_pre/MSRA/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:15,325 - __main__ - INFO - Load 20864 instances from /opt/ml/input/data/training/duuie_pre/PEOPLE_DAILY/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:26,197 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_中标/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:37,133 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_企业融资/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:47,968 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_股份回购/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:27:59,121 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_中标/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:28:10,279 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_高管变动/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:28:21,068 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_亏损/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:28:32,079 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_公司上市/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:28:42,968 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_被约谈/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:28:53,945 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_企业收购/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:05,082 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_股东减持/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:16,099 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_解除质押/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:28,293 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_企业破产/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:39,471 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_股东增持/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,010 - __main__ - INFO - Load 7015 instances from /opt/ml/input/data/training/duuie_pre/金融信息_质押/train.json\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,045 - __main__ - INFO - Meta Sample Negative: -1, Ordered SSI: False\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO - ********** Running training **********\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Num examples = 329270\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Num Epochs = 10\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Device train batch size = 16\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Device eval  batch size = 128\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Total  train batch size (w. accumulation) = 16\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m2022-06-09 10:29:51,048 - __main__ - INFO -   Total optimization steps = 205800\u001b[0m\n",
      "\u001b[34m2022-06-09 10:31:25,202 - __main__ - INFO - global_steps 500/205800 - lr: 0.0000202462  loss: 0.2252850926\u001b[0m\n",
      "\u001b[34m2022-06-09 10:32:58,975 - __main__ - INFO - global_steps 1000/205800 - lr: 0.0000404924  loss: 0.0706013387\u001b[0m\n",
      "\u001b[34m2022-06-09 10:34:32,881 - __main__ - INFO - global_steps 1500/205800 - lr: 0.0000607386  loss: 0.0523154555\u001b[0m\n",
      "\u001b[34m2022-06-09 10:36:06,804 - __main__ - INFO - global_steps 2000/205800 - lr: 0.0000809848  loss: 0.0437409830\u001b[0m\n",
      "\u001b[34m2022-06-09 10:37:40,161 - __main__ - INFO - global_steps 2500/205800 - lr: 0.0001012310  loss: 0.0397365519\u001b[0m\n",
      "\u001b[34m2022-06-09 10:39:14,370 - __main__ - INFO - global_steps 3000/205800 - lr: 0.0001214772  loss: 0.0369165316\u001b[0m\n",
      "\u001b[34m2022-06-09 10:40:49,241 - __main__ - INFO - global_steps 3500/205800 - lr: 0.0001417234  loss: 0.0339325170\u001b[0m\n",
      "\u001b[34m2022-06-09 10:42:23,113 - __main__ - INFO - global_steps 4000/205800 - lr: 0.0001619695  loss: 0.0322811233\u001b[0m\n",
      "\u001b[34m2022-06-09 10:43:56,954 - __main__ - INFO - global_steps 4500/205800 - lr: 0.0001822157  loss: 0.0311549151\u001b[0m\n",
      "\u001b[34m2022-06-09 10:45:30,666 - __main__ - INFO - global_steps 5000/205800 - lr: 0.0002024619  loss: 0.0320391979\u001b[0m\n",
      "\u001b[34m2022-06-09 10:47:04,573 - __main__ - INFO - global_steps 5500/205800 - lr: 0.0002227081  loss: 0.0306121104\u001b[0m\n",
      "\u001b[34m2022-06-09 10:48:38,216 - __main__ - INFO - global_steps 6000/205800 - lr: 0.0002429543  loss: 0.0288227499\u001b[0m\n",
      "\u001b[34m2022-06-09 10:50:12,628 - __main__ - INFO - global_steps 6500/205800 - lr: 0.0002632005  loss: 0.0286285768\u001b[0m\n",
      "\u001b[34m2022-06-09 10:51:46,533 - __main__ - INFO - global_steps 7000/205800 - lr: 0.0002834467  loss: 0.0299412670\u001b[0m\n",
      "\u001b[34m2022-06-09 10:53:21,599 - __main__ - INFO - global_steps 7500/205800 - lr: 0.0003036929  loss: 0.0297384215\u001b[0m\n",
      "\u001b[34m2022-06-09 10:54:55,614 - __main__ - INFO - global_steps 8000/205800 - lr: 0.0003239391  loss: 0.0276245778\u001b[0m\n",
      "\u001b[34m2022-06-09 10:56:28,672 - __main__ - INFO - global_steps 8500/205800 - lr: 0.0003441853  loss: 0.0298027554\u001b[0m\n",
      "\u001b[34m2022-06-09 10:58:03,031 - __main__ - INFO - global_steps 9000/205800 - lr: 0.0003644315  loss: 0.0296239432\u001b[0m\n",
      "\u001b[34m2022-06-09 10:59:37,887 - __main__ - INFO - global_steps 9500/205800 - lr: 0.0003846777  loss: 0.0304784750\u001b[0m\n",
      "\u001b[34m2022-06-09 11:01:12,343 - __main__ - INFO - global_steps 10000/205800 - lr: 0.0004049239  loss: 0.0293037898\u001b[0m\n",
      "\u001b[34m2022-06-09 11:02:46,173 - __main__ - INFO - global_steps 10500/205800 - lr: 0.0004251701  loss: 0.0285920131\u001b[0m\n",
      "\u001b[34m2022-06-09 11:04:19,840 - __main__ - INFO - global_steps 11000/205800 - lr: 0.0004454163  loss: 0.0287101833\u001b[0m\n",
      "\u001b[34m2022-06-09 11:05:54,735 - __main__ - INFO - global_steps 11500/205800 - lr: 0.0004656625  loss: 0.0308677090\u001b[0m\n",
      "\u001b[34m2022-06-09 11:07:30,178 - __main__ - INFO - global_steps 12000/205800 - lr: 0.0004859086  loss: 0.0294151274\u001b[0m\n",
      "\u001b[34m2022-06-09 11:09:04,566 - __main__ - INFO - global_steps 12500/205800 - lr: 0.0004996071  loss: 0.0306028073\u001b[0m\n",
      "\u001b[34m2022-06-09 11:10:38,852 - __main__ - INFO - global_steps 13000/205800 - lr: 0.0004983148  loss: 0.0287616552\u001b[0m\n",
      "\u001b[34m2022-06-09 11:12:12,403 - __main__ - INFO - global_steps 13500/205800 - lr: 0.0004970225  loss: 0.0289920699\u001b[0m\n",
      "\u001b[34m2022-06-09 11:13:46,457 - __main__ - INFO - global_steps 14000/205800 - lr: 0.0004957302  loss: 0.0285063863\u001b[0m\n",
      "\u001b[34m2022-06-09 11:15:19,852 - __main__ - INFO - global_steps 14500/205800 - lr: 0.0004944379  loss: 0.0295151474\u001b[0m\n",
      "\u001b[34m2022-06-09 11:16:53,991 - __main__ - INFO - global_steps 15000/205800 - lr: 0.0004931456  loss: 0.0285444720\u001b[0m\n",
      "\u001b[34m2022-06-09 11:18:29,404 - __main__ - INFO - global_steps 15500/205800 - lr: 0.0004918533  loss: 0.0287011434\u001b[0m\n",
      "\u001b[34m2022-06-09 11:20:03,494 - __main__ - INFO - global_steps 16000/205800 - lr: 0.0004905610  loss: 0.0280986407\u001b[0m\n",
      "\u001b[34m2022-06-09 11:21:40,059 - __main__ - INFO - global_steps 16500/205800 - lr: 0.0004892687  loss: 0.0275395067\u001b[0m\n",
      "\u001b[34m2022-06-09 11:23:13,514 - __main__ - INFO - global_steps 17000/205800 - lr: 0.0004879763  loss: 0.0268124149\u001b[0m\n",
      "\u001b[34m2022-06-09 11:24:48,247 - __main__ - INFO - global_steps 17500/205800 - lr: 0.0004866840  loss: 0.0272678548\u001b[0m\n",
      "\u001b[34m2022-06-09 11:26:21,950 - __main__ - INFO - global_steps 18000/205800 - lr: 0.0004853917  loss: 0.0278802203\u001b[0m\n",
      "\u001b[34m2022-06-09 11:27:56,202 - __main__ - INFO - global_steps 18500/205800 - lr: 0.0004840994  loss: 0.0258815411\u001b[0m\n",
      "\u001b[34m2022-06-09 11:29:30,408 - __main__ - INFO - global_steps 19000/205800 - lr: 0.0004828071  loss: 0.0269209068\u001b[0m\n",
      "\u001b[34m2022-06-09 11:31:03,320 - __main__ - INFO - global_steps 19500/205800 - lr: 0.0004815148  loss: 0.0241686026\u001b[0m\n",
      "\u001b[34m2022-06-09 11:32:37,998 - __main__ - INFO - global_steps 20000/205800 - lr: 0.0004802225  loss: 0.0251206391\u001b[0m\n",
      "\u001b[34m2022-06-09 11:34:12,495 - __main__ - INFO - global_steps 20500/205800 - lr: 0.0004789302  loss: 0.0251689212\u001b[0m\n",
      "\u001b[34m2022-06-09 11:34:27,446 - __main__ - INFO - saving checkpoint to /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch0\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 11:34:28,189] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch0/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 11:34:28,189] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch0/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 11:34:28,189] [    INFO]#033[0m - added tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch0/added_tokens.json#033[0m\u001b[0m\n",
      "\u001b[34m2022-06-09 11:35:46,723 - __main__ - INFO - global_steps 21000/205800 - lr: 0.0004776379  loss: 0.0231356945\u001b[0m\n",
      "\u001b[34m2022-06-09 11:37:21,503 - __main__ - INFO - global_steps 21500/205800 - lr: 0.0004763456  loss: 0.0225960640\u001b[0m\n",
      "\u001b[34m2022-06-09 11:38:55,456 - __main__ - INFO - global_steps 22000/205800 - lr: 0.0004750532  loss: 0.0227289426\u001b[0m\n",
      "\u001b[34m2022-06-09 11:40:30,183 - __main__ - INFO - global_steps 22500/205800 - lr: 0.0004737609  loss: 0.0228303168\u001b[0m\n",
      "\u001b[34m2022-06-09 11:42:06,224 - __main__ - INFO - global_steps 23000/205800 - lr: 0.0004724686  loss: 0.0229482491\u001b[0m\n",
      "\u001b[34m2022-06-09 11:43:40,575 - __main__ - INFO - global_steps 23500/205800 - lr: 0.0004711763  loss: 0.0225876503\u001b[0m\n",
      "\u001b[34m2022-06-09 11:45:14,307 - __main__ - INFO - global_steps 24000/205800 - lr: 0.0004698840  loss: 0.0213284107\u001b[0m\n",
      "\u001b[34m2022-06-09 11:46:49,112 - __main__ - INFO - global_steps 24500/205800 - lr: 0.0004685917  loss: 0.0221720229\u001b[0m\n",
      "\u001b[34m2022-06-09 11:48:23,253 - __main__ - INFO - global_steps 25000/205800 - lr: 0.0004672994  loss: 0.0221805050\u001b[0m\n",
      "\u001b[34m2022-06-09 11:49:57,262 - __main__ - INFO - global_steps 25500/205800 - lr: 0.0004660071  loss: 0.0230481553\u001b[0m\n",
      "\u001b[34m2022-06-09 11:51:30,447 - __main__ - INFO - global_steps 26000/205800 - lr: 0.0004647148  loss: 0.0220970531\u001b[0m\n",
      "\u001b[34m2022-06-09 11:53:03,781 - __main__ - INFO - global_steps 26500/205800 - lr: 0.0004634225  loss: 0.0216321037\u001b[0m\n",
      "\u001b[34m2022-06-09 11:54:38,943 - __main__ - INFO - global_steps 27000/205800 - lr: 0.0004621301  loss: 0.0225665687\u001b[0m\n",
      "\u001b[34m2022-06-09 11:56:12,814 - __main__ - INFO - global_steps 27500/205800 - lr: 0.0004608378  loss: 0.0224386306\u001b[0m\n",
      "\u001b[34m2022-06-09 11:57:46,651 - __main__ - INFO - global_steps 28000/205800 - lr: 0.0004595455  loss: 0.0218380470\u001b[0m\n",
      "\u001b[34m2022-06-09 11:59:20,804 - __main__ - INFO - global_steps 28500/205800 - lr: 0.0004582532  loss: 0.0221101141\u001b[0m\n",
      "\u001b[34m2022-06-09 12:00:55,552 - __main__ - INFO - global_steps 29000/205800 - lr: 0.0004569609  loss: 0.0212434349\u001b[0m\n",
      "\u001b[34m2022-06-09 12:02:30,312 - __main__ - INFO - global_steps 29500/205800 - lr: 0.0004556686  loss: 0.0218144090\u001b[0m\n",
      "\u001b[34m2022-06-09 12:04:04,923 - __main__ - INFO - global_steps 30000/205800 - lr: 0.0004543763  loss: 0.0209009478\u001b[0m\n",
      "\u001b[34m2022-06-09 12:05:38,770 - __main__ - INFO - global_steps 30500/205800 - lr: 0.0004530840  loss: 0.0221520380\u001b[0m\n",
      "\u001b[34m2022-06-09 12:07:13,064 - __main__ - INFO - global_steps 31000/205800 - lr: 0.0004517917  loss: 0.0212440588\u001b[0m\n",
      "\u001b[34m2022-06-09 12:08:47,599 - __main__ - INFO - global_steps 31500/205800 - lr: 0.0004504993  loss: 0.0204019162\u001b[0m\n",
      "\u001b[34m2022-06-09 12:10:21,532 - __main__ - INFO - global_steps 32000/205800 - lr: 0.0004492070  loss: 0.0208350490\u001b[0m\n",
      "\u001b[34m2022-06-09 12:11:55,386 - __main__ - INFO - global_steps 32500/205800 - lr: 0.0004479147  loss: 0.0207012826\u001b[0m\n",
      "\u001b[34m2022-06-09 12:13:29,132 - __main__ - INFO - global_steps 33000/205800 - lr: 0.0004466224  loss: 0.0204367133\u001b[0m\n",
      "\u001b[34m2022-06-09 12:15:02,310 - __main__ - INFO - global_steps 33500/205800 - lr: 0.0004453301  loss: 0.0196190337\u001b[0m\n",
      "\u001b[34m2022-06-09 12:16:36,982 - __main__ - INFO - global_steps 34000/205800 - lr: 0.0004440378  loss: 0.0208603899\u001b[0m\n",
      "\u001b[34m2022-06-09 12:18:10,790 - __main__ - INFO - global_steps 34500/205800 - lr: 0.0004427455  loss: 0.0201273901\u001b[0m\n",
      "\u001b[34m2022-06-09 12:19:45,513 - __main__ - INFO - global_steps 35000/205800 - lr: 0.0004414532  loss: 0.0205151271\u001b[0m\n",
      "\u001b[34m2022-06-09 12:21:19,661 - __main__ - INFO - global_steps 35500/205800 - lr: 0.0004401609  loss: 0.0207604172\u001b[0m\n",
      "\u001b[34m2022-06-09 12:22:54,154 - __main__ - INFO - global_steps 36000/205800 - lr: 0.0004388686  loss: 0.0201896109\u001b[0m\n",
      "\u001b[34m2022-06-09 12:24:27,964 - __main__ - INFO - global_steps 36500/205800 - lr: 0.0004375762  loss: 0.0204115295\u001b[0m\n",
      "\u001b[34m2022-06-09 12:26:02,345 - __main__ - INFO - global_steps 37000/205800 - lr: 0.0004362839  loss: 0.0201082702\u001b[0m\n",
      "\u001b[34m2022-06-09 12:27:36,377 - __main__ - INFO - global_steps 37500/205800 - lr: 0.0004349916  loss: 0.0195641587\u001b[0m\n",
      "\u001b[34m2022-06-09 12:29:10,919 - __main__ - INFO - global_steps 38000/205800 - lr: 0.0004336993  loss: 0.0196856417\u001b[0m\n",
      "\u001b[34m2022-06-09 12:30:45,164 - __main__ - INFO - global_steps 38500/205800 - lr: 0.0004324070  loss: 0.0192605548\u001b[0m\n",
      "\u001b[34m2022-06-09 12:32:19,150 - __main__ - INFO - global_steps 39000/205800 - lr: 0.0004311147  loss: 0.0190009387\u001b[0m\n",
      "\u001b[34m2022-06-09 12:33:53,386 - __main__ - INFO - global_steps 39500/205800 - lr: 0.0004298224  loss: 0.0187697592\u001b[0m\n",
      "\u001b[34m2022-06-09 12:35:26,909 - __main__ - INFO - global_steps 40000/205800 - lr: 0.0004285301  loss: 0.0193209906\u001b[0m\n",
      "\u001b[34m2022-06-09 12:37:01,473 - __main__ - INFO - global_steps 40500/205800 - lr: 0.0004272378  loss: 0.0194015565\u001b[0m\n",
      "\u001b[34m2022-06-09 12:38:36,056 - __main__ - INFO - global_steps 41000/205800 - lr: 0.0004259455  loss: 0.0202224554\u001b[0m\n",
      "\u001b[34m2022-06-09 12:39:06,314 - __main__ - INFO - saving checkpoint to /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch1\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 12:39:07,067] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch1/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 12:39:07,067] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch1/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 12:39:07,068] [    INFO]#033[0m - added tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch1/added_tokens.json#033[0m\u001b[0m\n",
      "\u001b[34m2022-06-09 12:40:11,926 - __main__ - INFO - global_steps 41500/205800 - lr: 0.0004246531  loss: 0.0166839944\u001b[0m\n",
      "\u001b[34m2022-06-09 12:41:46,571 - __main__ - INFO - global_steps 42000/205800 - lr: 0.0004233608  loss: 0.0169212543\u001b[0m\n",
      "\u001b[34m2022-06-09 12:43:21,061 - __main__ - INFO - global_steps 42500/205800 - lr: 0.0004220685  loss: 0.0161214063\u001b[0m\n",
      "\u001b[34m2022-06-09 12:44:54,836 - __main__ - INFO - global_steps 43000/205800 - lr: 0.0004207762  loss: 0.0159550142\u001b[0m\n",
      "\u001b[34m2022-06-09 12:46:28,376 - __main__ - INFO - global_steps 43500/205800 - lr: 0.0004194839  loss: 0.0159176252\u001b[0m\n",
      "\u001b[34m2022-06-09 12:48:03,075 - __main__ - INFO - global_steps 44000/205800 - lr: 0.0004181916  loss: 0.0172515931\u001b[0m\n",
      "\u001b[34m2022-06-09 12:49:37,250 - __main__ - INFO - global_steps 44500/205800 - lr: 0.0004168993  loss: 0.0166807122\u001b[0m\n",
      "\u001b[34m2022-06-09 12:51:11,326 - __main__ - INFO - global_steps 45000/205800 - lr: 0.0004156070  loss: 0.0159408403\u001b[0m\n",
      "\u001b[34m2022-06-09 12:52:45,883 - __main__ - INFO - global_steps 45500/205800 - lr: 0.0004143147  loss: 0.0159272657\u001b[0m\n",
      "\u001b[34m2022-06-09 12:54:19,763 - __main__ - INFO - global_steps 46000/205800 - lr: 0.0004130224  loss: 0.0167335024\u001b[0m\n",
      "\u001b[34m2022-06-09 12:55:54,260 - __main__ - INFO - global_steps 46500/205800 - lr: 0.0004117300  loss: 0.0159125494\u001b[0m\n",
      "\u001b[34m2022-06-09 12:57:28,818 - __main__ - INFO - global_steps 47000/205800 - lr: 0.0004104377  loss: 0.0167836073\u001b[0m\n",
      "\u001b[34m2022-06-09 12:59:02,592 - __main__ - INFO - global_steps 47500/205800 - lr: 0.0004091454  loss: 0.0160403004\u001b[0m\n",
      "\u001b[34m2022-06-09 13:00:36,199 - __main__ - INFO - global_steps 48000/205800 - lr: 0.0004078531  loss: 0.0156851031\u001b[0m\n",
      "\u001b[34m2022-06-09 13:02:10,360 - __main__ - INFO - global_steps 48500/205800 - lr: 0.0004065608  loss: 0.0160444100\u001b[0m\n",
      "\u001b[34m2022-06-09 13:03:44,834 - __main__ - INFO - global_steps 49000/205800 - lr: 0.0004052685  loss: 0.0159316636\u001b[0m\n",
      "\u001b[34m2022-06-09 13:05:20,256 - __main__ - INFO - global_steps 49500/205800 - lr: 0.0004039762  loss: 0.0162585925\u001b[0m\n",
      "\u001b[34m2022-06-09 13:06:54,985 - __main__ - INFO - global_steps 50000/205800 - lr: 0.0004026839  loss: 0.0165108432\u001b[0m\n",
      "\u001b[34m2022-06-09 13:08:28,667 - __main__ - INFO - global_steps 50500/205800 - lr: 0.0004013916  loss: 0.0156351475\u001b[0m\n",
      "\u001b[34m2022-06-09 13:10:03,525 - __main__ - INFO - global_steps 51000/205800 - lr: 0.0004000992  loss: 0.0147634307\u001b[0m\n",
      "\u001b[34m2022-06-09 13:11:38,602 - __main__ - INFO - global_steps 51500/205800 - lr: 0.0003988069  loss: 0.0156380901\u001b[0m\n",
      "\u001b[34m2022-06-09 13:13:13,200 - __main__ - INFO - global_steps 52000/205800 - lr: 0.0003975146  loss: 0.0158124989\u001b[0m\n",
      "\u001b[34m2022-06-09 13:14:46,951 - __main__ - INFO - global_steps 52500/205800 - lr: 0.0003962223  loss: 0.0150451850\u001b[0m\n",
      "\u001b[34m2022-06-09 13:16:21,151 - __main__ - INFO - global_steps 53000/205800 - lr: 0.0003949300  loss: 0.0150260176\u001b[0m\n",
      "\u001b[34m2022-06-09 13:17:55,397 - __main__ - INFO - global_steps 53500/205800 - lr: 0.0003936377  loss: 0.0155081139\u001b[0m\n",
      "\u001b[34m2022-06-09 13:19:30,479 - __main__ - INFO - global_steps 54000/205800 - lr: 0.0003923454  loss: 0.0159161660\u001b[0m\n",
      "\u001b[34m2022-06-09 13:21:05,074 - __main__ - INFO - global_steps 54500/205800 - lr: 0.0003910531  loss: 0.0159307794\u001b[0m\n",
      "\u001b[34m2022-06-09 13:22:39,453 - __main__ - INFO - global_steps 55000/205800 - lr: 0.0003897608  loss: 0.0157163593\u001b[0m\n",
      "\u001b[34m2022-06-09 13:24:14,433 - __main__ - INFO - global_steps 55500/205800 - lr: 0.0003884685  loss: 0.0155829802\u001b[0m\n",
      "\u001b[34m2022-06-09 13:25:48,119 - __main__ - INFO - global_steps 56000/205800 - lr: 0.0003871761  loss: 0.0155598447\u001b[0m\n",
      "\u001b[34m2022-06-09 13:27:22,340 - __main__ - INFO - global_steps 56500/205800 - lr: 0.0003858838  loss: 0.0160177106\u001b[0m\n",
      "\u001b[34m2022-06-09 13:28:56,328 - __main__ - INFO - global_steps 57000/205800 - lr: 0.0003845915  loss: 0.0148815012\u001b[0m\n",
      "\u001b[34m2022-06-09 13:30:30,880 - __main__ - INFO - global_steps 57500/205800 - lr: 0.0003832992  loss: 0.0152514372\u001b[0m\n",
      "\u001b[34m2022-06-09 13:32:05,377 - __main__ - INFO - global_steps 58000/205800 - lr: 0.0003820069  loss: 0.0145071773\u001b[0m\n",
      "\u001b[34m2022-06-09 13:33:39,321 - __main__ - INFO - global_steps 58500/205800 - lr: 0.0003807146  loss: 0.0151664778\u001b[0m\n",
      "\u001b[34m2022-06-09 13:35:13,721 - __main__ - INFO - global_steps 59000/205800 - lr: 0.0003794223  loss: 0.0148935457\u001b[0m\n",
      "\u001b[34m2022-06-09 13:36:48,909 - __main__ - INFO - global_steps 59500/205800 - lr: 0.0003781300  loss: 0.0147550898\u001b[0m\n",
      "\u001b[34m2022-06-09 13:38:23,062 - __main__ - INFO - global_steps 60000/205800 - lr: 0.0003768377  loss: 0.0153564663\u001b[0m\n",
      "\u001b[34m2022-06-09 13:39:56,671 - __main__ - INFO - global_steps 60500/205800 - lr: 0.0003755454  loss: 0.0153218916\u001b[0m\n",
      "\u001b[34m2022-06-09 13:41:30,074 - __main__ - INFO - global_steps 61000/205800 - lr: 0.0003742530  loss: 0.0149476128\u001b[0m\n",
      "\u001b[34m2022-06-09 13:43:04,390 - __main__ - INFO - global_steps 61500/205800 - lr: 0.0003729607  loss: 0.0147919088\u001b[0m\n",
      "\u001b[34m2022-06-09 13:43:49,734 - __main__ - INFO - saving checkpoint to /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch2\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 13:43:50,466] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch2/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 13:43:50,467] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch2/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-06-09 13:43:50,467] [    INFO]#033[0m - added tokens file saved in /opt/ml/model/duuie_multi_task_b32_lr5e-4/ckpt_epoch2/added_tokens.json#033[0m\u001b[0m\n",
      "\u001b[34m2022-06-09 13:44:39,687 - __main__ - INFO - global_steps 62000/205800 - lr: 0.0003716684  loss: 0.0129718766\u001b[0m\n",
      "\u001b[34m2022-06-09 13:46:14,036 - __main__ - INFO - global_steps 62500/205800 - lr: 0.0003703761  loss: 0.0126246711\u001b[0m\n",
      "\u001b[34m2022-06-09 13:47:48,026 - __main__ - INFO - global_steps 63000/205800 - lr: 0.0003690838  loss: 0.0124375809\u001b[0m\n",
      "\u001b[34m2022-06-09 13:49:21,920 - __main__ - INFO - global_steps 63500/205800 - lr: 0.0003677915  loss: 0.0129448308\u001b[0m\n",
      "\u001b[34m2022-06-09 13:50:56,918 - __main__ - INFO - global_steps 64000/205800 - lr: 0.0003664992  loss: 0.0127462417\u001b[0m\n",
      "\u001b[34m2022-06-09 13:52:30,788 - __main__ - INFO - global_steps 64500/205800 - lr: 0.0003652069  loss: 0.0123327716\u001b[0m\n",
      "\u001b[34m2022-06-09 13:54:04,098 - __main__ - INFO - global_steps 65000/205800 - lr: 0.0003639146  loss: 0.0130310640\u001b[0m\n",
      "\u001b[34m2022-06-09 13:55:39,535 - __main__ - INFO - global_steps 65500/205800 - lr: 0.0003626223  loss: 0.0120393462\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {'multi_task_config': '/opt/ml/input/data/training/config/multi-task-duuie.yaml',\n",
    "                   'negative_keep': 1.0,\n",
    "                   'do_train': '',\n",
    "                   'metric_for_best_model': 'all-task-ave',\n",
    "                   'model_name_or_path': '/opt/ml/input/data/training/uie-char-small',\n",
    "                   'num_train_epochs': 10,\n",
    "                   'per_device_train_batch_size': 16,  # 32\n",
    "                   'per_device_eval_batch_size': 128,  # 256\n",
    "                   'output_dir': '/opt/ml/model/duuie_multi_task_b32_lr5e-4',\n",
    "                   'logging_dir': '/opt/ml/output/duuie_multi_task_b32_lr5e-4_log',\n",
    "                   'learning_rate': 5e-4,\n",
    "                   'overwrite_output_dir': '',\n",
    "                   'gradient_accumulation_steps': 1,\n",
    "                   'device': 'gpu'}\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'  # 'ml.p3.2xlarge' or 'ml.p3.8xlarge' or ...\n",
    "\n",
    "# git_config = {'repo': 'https://github.com/PaddlePaddle/PaddleNLP.git', 'branch': 'develop'}\n",
    "\n",
    "estimator = PyTorch(entry_point='run_seq2struct.py',\n",
    "                    source_dir='./',\n",
    "#                             source_dir='model_zoo/uie/',\n",
    "#                             git_config=git_config,\n",
    "                            role=role,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            framework_version='1.9.1',\n",
    "                            py_version='py38',\n",
    "                            script_mode=True,\n",
    "                            instance_count=1,  # 1 or 2 or ...\n",
    "                            instance_type=instance_type)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "# training_job_name = 'pytorch-training-2022-06-07-03-39-32-658'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint (in this case locally) which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!rm -rf model_*\n",
    "!rm -rf inference.*\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz .\n",
    "!tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp inference.* model/\n",
    "!cd model && tar -czvf ../model-inference.tar.gz *\n",
    "\n",
    "!aws s3 cp model-inference.tar.gz s3://$bucket/$training_job_name/output/model-inference.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instance_type = 'local'\n",
    "instance_type = 'ml.m5.xlarge'\n",
    "\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://{}/{}/output/model-inference.tar.gz'.format(bucket, training_job_name), role=role,\n",
    "                             entry_point='infer.py', framework_version='1.9.1', py_version='py38')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "endpointName = 'pytorch-inference-2022-06-07-08-04-23-851'\n",
    "\n",
    "predictor = Predictor(endpointName, sagemaker_session=sagemaker_session, serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [{'行业': [{'text': '律师', 'start': 53, 'end': 55, 'probability': 0.5180757641792297}, {'text': '开发公司', 'start': 77, 'end': 81, 'probability': 0.5001093149185181}], '地域': [{'text': '北京市', 'start': 48, 'end': 51, 'probability': 0.6983951330184937}, {'text': '北京市', 'start': 1, 'end': 4, 'probability': 0.5646492838859558}], '组织形式': [{'text': '开发公司', 'start': 77, 'end': 81, 'probability': 0.4969618618488312}, {'text': '法院', 'start': 9, 'end': 11, 'probability': 0.5218775868415833}], '商号': [{'text': '建初', 'start': 24, 'end': 26, 'probability': 0.6158205270767212}]}, {'地域': [{'text': '山东', 'start': 64, 'end': 66, 'probability': 0.8744303584098816}, {'text': '深圳市', 'start': 25, 'end': 28, 'probability': 0.9532861113548279}], '组织形式': [{'text': '事务所', 'start': 31, 'end': 34, 'probability': 0.5747163891792297}], '商号': [{'text': 'C', 'start': 28, 'end': 29, 'probability': 0.5181689858436584}]}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "texts = ['\"北京市海淀区人民法院\\n民事判决书\\n(199x)建初字第xxx号\\n原告：张三。\\n委托代理人李四，北京市 A律师事务所律师。\\n被告：B公司，法定代表人王五，开发公司总经理。\\n委托代理人赵六，北京市 C律师事务所律师。\"', \n",
    "         '原告赵六，2022年5月29日生\\n委托代理人孙七，深圳市C律师事务所律师。\\n被告周八，1990年7月28日出生\\n委托代理人吴九，山东D律师事务所律师']\n",
    "\n",
    "outputs = predictor.predict(texts)\n",
    "print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
